{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2546b39-7738-4b10-aa8c-c479f4dc3081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/Volumes/workspace/default/netflix/netflix_titles.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dec0364-4315-4a19-b2dc-09ae8a00efc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- show_id: string (nullable = true)\n |-- type: string (nullable = true)\n |-- title: string (nullable = true)\n |-- director: string (nullable = true)\n |-- cast: string (nullable = true)\n |-- country: string (nullable = true)\n |-- date_added: string (nullable = true)\n |-- release_year: string (nullable = true)\n |-- rating: string (nullable = true)\n |-- duration: string (nullable = true)\n |-- listed_in: string (nullable = true)\n |-- description: string (nullable = true)\n\n+-------+-------+--------------------+---------------+--------------------+-------------+------------------+------------+------+---------+--------------------+--------------------+\n|show_id|   type|               title|       director|                cast|      country|        date_added|release_year|rating| duration|           listed_in|         description|\n+-------+-------+--------------------+---------------+--------------------+-------------+------------------+------------+------+---------+--------------------+--------------------+\n|     s1|  Movie|Dick Johnson Is Dead|Kirsten Johnson|                NULL|United States|September 25, 2021|        2020| PG-13|   90 min|       Documentaries|As her father nea...|\n|     s2|TV Show|       Blood & Water|           NULL|Ama Qamata, Khosi...| South Africa|September 24, 2021|        2021| TV-MA|2 Seasons|International TV ...|After crossing pa...|\n|     s3|TV Show|           Ganglands|Julien Leclercq|Sami Bouajila, Tr...|         NULL|September 24, 2021|        2021| TV-MA| 1 Season|Crime TV Shows, I...|To protect his fa...|\n|     s4|TV Show|Jailbirds New Orl...|           NULL|                NULL|         NULL|September 24, 2021|        2021| TV-MA| 1 Season|Docuseries, Reali...|Feuds, flirtation...|\n|     s5|TV Show|        Kota Factory|           NULL|Mayur More, Jiten...|        India|September 24, 2021|        2021| TV-MA|2 Seasons|International TV ...|In a city of coac...|\n+-------+-------+--------------------+---------------+--------------------+-------------+------------------+------------+------+---------+--------------------+--------------------+\nonly showing top 5 rows\nInitial dataset shape: 8809 rows, 12 columns\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)\n",
    "print(f\"Initial dataset shape: {df.count()} rows, {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20272239-fc9e-49e1-bb88-c1aeeb0683c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n+-------+----+-----+--------+----+-------+----------+------------+------+--------+---------+-----------+\n|show_id|type|title|director|cast|country|date_added|release_year|rating|duration|listed_in|description|\n+-------+----+-----+--------+----+-------+----------+------------+------+--------+---------+-----------+\n|      0|   1|    2|    2636| 826|    832|        13|           2|     6|       5|        3|          3|\n+-------+----+-----+--------+----+-------+----------+------------+------+--------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "missing_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "print(\"Missing values per column:\")\n",
    "missing_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124433dc-d774-4cef-ac13-ed7b40cd40c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicate rows\n"
     ]
    }
   ],
   "source": [
    "initial_count = df.count()\n",
    "df = df.dropDuplicates()\n",
    "final_count = df.count()\n",
    "print(f\"Removed {initial_count - final_count} duplicate rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33410ca5-9636-4879-a769-2314dc60a17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handled for country, director, cast, and rating\n"
     ]
    }
   ],
   "source": [
    "df = df.fillna({\n",
    "    \"country\": \"Unknown\",\n",
    "    \"director\": \"Unknown\", \n",
    "    \"cast\": \"Unknown\",\n",
    "    \"rating\": \"Not Rated\"\n",
    "})\n",
    "print(\"Missing values handled for country, director, cast, and rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f784c70-342b-4a05-b1b4-047d8eb8a9df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text columns cleaned and standardized\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim, lower, col, initcap\n",
    "\n",
    "df = df.withColumn(\"title\", trim(col(\"title\"))) \\\n",
    "       .withColumn(\"type\", trim(lower(col(\"type\")))) \\\n",
    "       .withColumn(\"country\", trim(initcap(col(\"country\")))) \\\n",
    "       .withColumn(\"rating\", trim(col(\"rating\"))) \\\n",
    "       .withColumn(\"director\", trim(col(\"director\"))) \\\n",
    "       .withColumn(\"cast\", trim(col(\"cast\"))) \\\n",
    "       .withColumn(\"listed_in\", trim(col(\"listed_in\"))) \\\n",
    "       .withColumn(\"date_added\", trim(col(\"date_added\")))  # Just clean, don't convert\n",
    "\n",
    "print(\"Text columns cleaned and standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4ab699-7fed-4ba9-a6f4-cdd3efdb3f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration and release year converted successfully\n+---------+------------+-------------+------------+\n| duration|duration_int|duration_type|release_year|\n+---------+------------+-------------+------------+\n|  104 min|         104|      minutes|        2016|\n|  135 min|         135|      minutes|        2013|\n|3 Seasons|           3|      seasons|        2020|\n| 1 Season|           1|      seasons|        2009|\n|   58 min|          58|      minutes|        2021|\n| 1 Season|           1|      seasons|        2018|\n| 1 Season|           1|      seasons|        2017|\n|  106 min|         106|      minutes|        2021|\n|   47 min|          47|      minutes|        2021|\n|5 Seasons|           5|      seasons|        2021|\n+---------+------------+-------------+------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, when, regexp_extract\n",
    "df = df.withColumn(\"duration_int\", \n",
    "    expr(\"try_cast(regexp_extract(duration, '([0-9]+)', 1) AS INT)\"))\n",
    "\n",
    "df = df.withColumn(\"duration_type\", \n",
    "    when(col(\"duration\").contains(\"min\"), \"minutes\")\n",
    "    .when(col(\"duration\").contains(\"Season\"), \"seasons\")\n",
    "    .otherwise(\"unknown\"))\n",
    "\n",
    "df = df.withColumn(\"release_year\", expr(\"try_cast(trim(release_year) AS INT)\"))\n",
    "\n",
    "print(\"Duration and release year converted successfully\")\n",
    "df.select(\"duration\", \"duration_int\", \"duration_type\", \"release_year\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3aa1ae7-6910-4d9b-bf1c-42fbbb2caecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted year and month from date_added (kept original as string)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Extract year from date_added string (safer than date parsing)\n",
    "df = df.withColumn(\"added_year\", \n",
    "    expr(\"try_cast(regexp_extract(date_added, '([0-9]{4})', 1) AS INT)\"))\n",
    "\n",
    "# Extract month name and convert to number\n",
    "df = df.withColumn(\"added_month\", \n",
    "    when(col(\"date_added\").contains(\"January\"), 1)\n",
    "    .when(col(\"date_added\").contains(\"February\"), 2)\n",
    "    .when(col(\"date_added\").contains(\"March\"), 3)\n",
    "    .when(col(\"date_added\").contains(\"April\"), 4)\n",
    "    .when(col(\"date_added\").contains(\"May\"), 5)\n",
    "    .when(col(\"date_added\").contains(\"June\"), 6)\n",
    "    .when(col(\"date_added\").contains(\"July\"), 7)\n",
    "    .when(col(\"date_added\").contains(\"August\"), 8)\n",
    "    .when(col(\"date_added\").contains(\"September\"), 9)\n",
    "    .when(col(\"date_added\").contains(\"October\"), 10)\n",
    "    .when(col(\"date_added\").contains(\"November\"), 11)\n",
    "    .when(col(\"date_added\").contains(\"December\"), 12)\n",
    "    .otherwise(None))\n",
    "\n",
    "print(\"Extracted year and month from date_added (kept original as string)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee92e20-2f11-431c-9b13-fc63e7e86291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying data validation filters...\nRemoved 25 invalid records\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, year\n",
    "\n",
    "# Get current year\n",
    "current_year = int(df.select(year(current_date())).first()[0])\n",
    "\n",
    "print(\"Applying data validation filters...\")\n",
    "before_filter = df.count()\n",
    "\n",
    "# Filter invalid release years\n",
    "df = df.filter((col(\"release_year\") >= 1900) & (col(\"release_year\") <= current_year))\n",
    "\n",
    "# Filter invalid durations\n",
    "df = df.filter(col(\"duration_int\").isNotNull() & (col(\"duration_int\") > 0))\n",
    "\n",
    "# Remove rows where essential fields are missing\n",
    "df = df.filter(col(\"title\").isNotNull() & (col(\"title\") != \"\"))\n",
    "\n",
    "# Filter invalid added_year\n",
    "df = df.filter(col(\"added_year\").isNull() | \n",
    "               ((col(\"added_year\") >= 2000) & (col(\"added_year\") <= current_year)))\n",
    "\n",
    "after_filter = df.count()\n",
    "print(f\"Removed {before_filter - after_filter} invalid records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae597c17-c2e7-42f0-9cc6-9d3256fa5b52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== DATA QUALITY SUMMARY ===\n\nFinal Schema:\nroot\n |-- show_id: string (nullable = true)\n |-- type: string (nullable = true)\n |-- title: string (nullable = true)\n |-- director: string (nullable = false)\n |-- cast: string (nullable = false)\n |-- country: string (nullable = false)\n |-- date_added: string (nullable = true)\n |-- release_year: integer (nullable = true)\n |-- rating: string (nullable = false)\n |-- duration: string (nullable = true)\n |-- listed_in: string (nullable = true)\n |-- description: string (nullable = true)\n |-- duration_int: integer (nullable = true)\n |-- duration_type: string (nullable = false)\n |-- added_year: integer (nullable = true)\n |-- added_month: integer (nullable = true)\n\n\nUnique content types:\n+-------+\n|   type|\n+-------+\n|tv show|\n|  movie|\n+-------+\n\n\nUnique ratings:\n+---------+\n|   rating|\n+---------+\n|    TV-14|\n|     TV-Y|\n|    TV-PG|\n|    TV-Y7|\n|        G|\n|    TV-MA|\n|        R|\n|       PG|\n|     TV-G|\n|    PG-13|\n|    NC-17|\n| TV-Y7-FV|\n|Not Rated|\n|       UR|\n|       NR|\n+---------+\n\n\nDuration types:\n+-------------+\n|duration_type|\n+-------------+\n|      seasons|\n|      minutes|\n+-------------+\n\n\nAdded year range:\n+-------+------------------+\n|summary|        added_year|\n+-------+------------------+\n|  count|              8774|\n|   mean|2018.8713243674492|\n| stddev|1.5733056564358028|\n|    min|              2008|\n|    max|              2021|\n+-------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nFinal Schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\nUnique content types:\")\n",
    "df.select(\"type\").distinct().show()\n",
    "\n",
    "print(\"\\nUnique ratings:\")\n",
    "df.select(\"rating\").distinct().show()\n",
    "\n",
    "print(\"\\nDuration types:\")\n",
    "df.select(\"duration_type\").distinct().show()\n",
    "\n",
    "print(\"\\nAdded year range:\")\n",
    "df.select(\"added_year\").describe().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd628b9-6c33-440f-ace8-33687734622e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== FINAL DATASET SUMMARY ===\nFinal dataset shape: 8784 rows, 16 columns\n\nSample of cleaned data:\n+--------------------+-------+------------+------+------------+----------+-----------+\n|               title|   type|release_year|rating|duration_int|added_year|added_month|\n+--------------------+-------+------------+------+------------+----------+-----------+\n|             Görümce|  movie|        2016| TV-MA|         104|      2020|          4|\n|     Chennai Express|  movie|        2013| TV-14|         135|      2021|          8|\n|The Last Kids on ...|tv show|        2020| TV-Y7|           3|      2020|         10|\n|   Boys Over Flowers|tv show|        2009| TV-14|           1|      2020|          5|\n|Searching For Sheela|  movie|        2021| TV-14|          58|      2021|          4|\n|  Peasants Rebellion|tv show|        2018| TV-14|           1|      2021|          4|\n|Strongest Deliver...|tv show|        2017| TV-MA|           1|      2020|         11|\n|You vs. Wild: Out...|  movie|        2021|  TV-G|         106|      2021|          9|\n|Little Singham Fu...|  movie|        2021| TV-Y7|          47|      2021|          5|\n|              Bunk'd|tv show|        2021|  TV-G|           5|      2021|          9|\n+--------------------+-------+------------+------+------------+----------+-----------+\nonly showing top 10 rows\nData cleaning completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== FINAL DATASET SUMMARY ===\")\n",
    "print(f\"Final dataset shape: {df.count()} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# Show sample of cleaned data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "df.select(\"title\", \"type\", \"release_year\", \"rating\", \"duration_int\", \"added_year\", \"added_month\").show(10)\n",
    "\n",
    "print(\"Data cleaning completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6055b668-11e7-416e-bbd8-39faaf68d97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').option(\"header\", \"true\").csv(\"/Volumes/workspace/default/netflix/cleaned_netflix_titles\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "netflix_clean_notebook_kt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}