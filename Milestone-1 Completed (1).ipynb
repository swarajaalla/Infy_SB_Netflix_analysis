{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce522b6-eff8-4c5f-bf2c-413a6cebee6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Databricks Notebook: Netflix Dataset Cleaning & Encoding\n",
    "\n",
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "# Step 2: Load Dataset\n",
    "df = pd.read_csv(\"/Volumes/workspace/default/netflix-1/netflix_titles.csv\")\n",
    "print(\"✅ Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08d88ea-b319-4433-99a0-1c48b66e9fbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Duplicate rows removed.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Remove Duplicate Rows\n",
    "df = df.drop_duplicates()\n",
    "print(\"✅ Duplicate rows removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bde121-3dc3-44f3-924c-c95ee847b6c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Missing values handled.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Handle Missing Values\n",
    "df['country'] = df['country'].fillna(\"unknown\")\n",
    "df['director'] = df['director'].fillna(\"Not Available\")\n",
    "df['cast'] = df['cast'].fillna(\"Not Available\")\n",
    "df['rating'] = df['rating'].fillna(\"Not Rated\")\n",
    "df['duration'] = df['duration'].fillna(\"0\")\n",
    "print(\"✅ Missing values handled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0452aa-7daf-4e4e-a165-944fc0fc48aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extra spaces removed from object columns.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Strip Extra Spaces from Strings\n",
    "for col in ['type', 'rating', 'country', 'director', 'listed_in']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "print(\"✅ Extra spaces removed from object columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d5ef65-4dfa-455e-a2f9-04498fc90c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Special characters removed from 'country'.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Remove Special Characters from 'country'\n",
    "df['country'] = df['country'].str.replace(r'[^a-zA-Z ,]', '', regex=True)\n",
    "print(\"✅ Special characters removed from 'country'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45903353-214f-4e70-8d70-054c266880a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Columns renamed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Rename Columns (if needed)\n",
    "df = df.rename(columns={\"show_id\": \"id\"})\n",
    "print(\"✅ Columns renamed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "417a2b5e-b78e-4ba8-a254-17d5cb104603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optional columns dropped if applied.\n✅ Removed 0 duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 8: Drop Unnecessary Columns (optional)\n",
    "# Uncomment below to drop 'description' column if desired\n",
    "# if 'description' in df.columns:\n",
    "#     df = df.drop(columns=['description'])\n",
    "print(\"✅ Optional columns dropped if applied.\")\n",
    "\n",
    "# Step 9: Handle Duplicates Again\n",
    "before = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "after = df.shape[0]\n",
    "print(f\"✅ Removed {before - after} duplicate rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa6cc62-298d-4f3e-8dc4-92a8f0680ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned dataset saved as 'cleaned_netflix_titles.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Create a Cleaned Copy and Save\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned.to_csv(\"/Volumes/workspace/default/netflix-1/cleaned_netflix_titles.csv\", index=False)\n",
    "print(\"✅ Cleaned dataset saved as 'cleaned_netflix_titles.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e78090-09ee-4811-8f41-6ef1a614e1b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'rating' column label encoded.\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Encoding 'rating' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df_cleaned['rating_encoded'] = label_encoder.fit_transform(df_cleaned['rating'])\n",
    "print(\"✅ 'rating' column label encoded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a3ab75a-b6da-4567-baac-05230ecee6ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frequency encoding applied to 'country'.\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Frequency Encoding for 'country'\n",
    "if 'country' in df_cleaned.columns:\n",
    "    country_freq = df_cleaned['country'].value_counts().to_dict()\n",
    "    df_cleaned['country_encoded'] = df_cleaned['country'].map(country_freq)\n",
    "    print(\"✅ Frequency encoding applied to 'country'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08bfda61-c268-4f56-aca6-7cfa094ac6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-label one-hot encoding applied to genres.\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Multi-label One-Hot Encoding for Genres\n",
    "df_cleaned['genres_list'] = df_cleaned['listed_in'].apply(lambda x: [g.strip() for g in str(x).split(',')])\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = pd.DataFrame(\n",
    "    mlb.fit_transform(df_cleaned['genres_list']),\n",
    "    columns=mlb.classes_,\n",
    "    index=df_cleaned.index\n",
    ")\n",
    "df_cleaned = pd.concat([df_cleaned, genres_encoded], axis=1)\n",
    "print(\"✅ Multi-label one-hot encoding applied to genres.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66df2ac4-bb77-4dfd-8737-0f603fc85e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frequency encoding applied to primary genre.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 14: Frequency Encoding for Primary Genre\n",
    "df_cleaned['primary_genre'] = df_cleaned['listed_in'].apply(lambda x: str(x).split(',')[0].strip())\n",
    "genre_freq = df_cleaned['primary_genre'].value_counts().to_dict()\n",
    "df_cleaned['genre_encoded'] = df_cleaned['primary_genre'].map(genre_freq)\n",
    "print(\"✅ Frequency encoding applied to primary genre.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe599a9a-943d-4ac3-8b8f-94360f6238c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frequency encoded dataset saved as 'freq_encoded_netflix_titles.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Step 15: Save Frequency Encoded Dataset\n",
    "df_cleaned.to_csv(\"/Volumes/workspace/default/netflix-1/freq_encoded_netflix_titles.csv\", index=False)\n",
    "print(\"✅ Frequency encoded dataset saved as 'freq_encoded_netflix_titles.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Milestone-1 Completed",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}